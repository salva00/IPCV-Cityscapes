{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport imageio.v2 as imageio\n\n# Questo è un esempio di un'immagine e la sua label\n\nimg = imageio.imread('/kaggle/input/cityscapes/Cityspaces/images/train/aachen/aachen_000000_000019_leftImg8bit.png')\nmask = imageio.imread('/kaggle/input/cityscapes/Cityspaces/gtFine/train/aachen/aachen_000000_000019_gtFine_labelIds.png')\n\n\nplt.figure()\nfig, axs = plt.subplots(1,2,sharex=True, sharey=True,figsize=(16,7))\naxs[0].imshow(img)\naxs[0].set_title('Immagine RGB', fontsize=20, fontweight='bold')\naxs[1].imshow(mask)\naxs[1].set_title('Ground Truth', fontsize=20, fontweight='bold')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:31:57.924342Z","iopub.execute_input":"2023-05-14T15:31:57.924934Z","iopub.status.idle":"2023-05-14T15:31:59.864105Z","shell.execute_reply.started":"2023-05-14T15:31:57.924904Z","shell.execute_reply":"2023-05-14T15:31:59.861582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-05-14T14:24:40.717802Z","iopub.execute_input":"2023-05-14T14:24:40.718057Z","iopub.status.idle":"2023-05-14T14:24:40.721537Z","shell.execute_reply.started":"2023-05-14T14:24:40.718036Z","shell.execute_reply":"2023-05-14T14:24:40.720939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python -m pip install cityscapesscripts","metadata":{"execution":{"iopub.status.busy":"2023-05-14T14:24:40.723267Z","iopub.execute_input":"2023-05-14T14:24:40.723501Z","iopub.status.idle":"2023-05-14T14:24:40.733643Z","shell.execute_reply.started":"2023-05-14T14:24:40.723482Z","shell.execute_reply":"2023-05-14T14:24:40.732689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nfrom torchvision import transforms as T\nfrom torchvision.transforms import functional as TF\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n#from torchsummary import summary\nfrom tqdm import tqdm\n#from cityscapesscripts.helpers.labels import trainId2label as t2l\nimport cv2\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:32:05.856853Z","iopub.execute_input":"2023-05-14T15:32:05.857214Z","iopub.status.idle":"2023-05-14T15:32:13.063303Z","shell.execute_reply.started":"2023-05-14T15:32:05.857186Z","shell.execute_reply":"2023-05-14T15:32:13.062096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CityscapesDataset(Dataset):\n    def __init__(self, image_dir, label_dir):\n        self.imagepaths=[]\n        self.labelpaths=[]\n        self.labelcolorpaths=[]\n        self.image_dir=image_dir\n        self.label_dir=label_dir\n\n        dir = [os.path.join(image_dir, dir) for dir in sorted(os.listdir(image_dir))]\n        for dir1 in sorted(dir):\n            self.imagepaths.extend([os.path.join(dir1, dir) for dir in sorted(os.listdir(dir1))]) \n\n        dir = [os.path.join(label_dir, dir) for dir in sorted(os.listdir(label_dir))]\n        for dir1 in dir:\n            labelpaths = [os.path.join(dir1, dir) for dir in sorted(os.listdir(dir1))]\n            for img in labelpaths:\n                if 'labelIds' in os.path.basename(img):\n                    self.labelpaths.append(img)\n\n        dir = [os.path.join(label_dir, dir) for dir in sorted(os.listdir(label_dir))]\n        for dir1 in dir:\n            labelpaths = [os.path.join(dir1, dir) for dir in sorted(os.listdir(dir1))]\n            for img in labelpaths:\n                if 'color' in os.path.basename(img):\n                    self.labelcolorpaths.append(img)              \n\n        \n        \n    def __len__(self):\n        return len(self.imagepaths)\n\n    def __getitem__(self, idx):\n                    \n        image = imageio.imread(self.imagepaths[idx])\n        mask = imageio.imread(self.labelpaths[idx])\n        mask_color = imageio.imread(self.labelcolorpaths[idx])\n        \n        image = cv2.resize(image, (220,110))\n        mask = cv2.resize(mask, (220,110))\n        mask_color = cv2.resize(mask_color, (220,110))\n\n        img = torch.tensor(image, dtype=torch.float32)\n        img = torch.tensor(img.tolist())\n        mask = torch.tensor(mask, dtype=torch.int8)\n        mask = torch.tensor(mask.tolist())\n        img = img.permute(2, 0, 1)\n        mask_color = torch.tensor(mask_color, dtype=torch.float32)\n        mask_color = torch.tensor(mask_color.tolist())\n        mask_color = mask_color.permute(2, 0, 1)\n        \n        return img, mask, mask_color    ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T17:06:16.442416Z","iopub.execute_input":"2023-05-14T17:06:16.442794Z","iopub.status.idle":"2023-05-14T17:06:16.458683Z","shell.execute_reply.started":"2023-05-14T17:06:16.442763Z","shell.execute_reply":"2023-05-14T17:06:16.457489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels=3, out_channels=64):\n        super(DoubleConv, self).__init__()\n        self.doubleconv=nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ) \n        \n    def forward(self, x):\n            return self.doubleconv(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:32:20.494714Z","iopub.execute_input":"2023-05-14T15:32:20.495147Z","iopub.status.idle":"2023-05-14T15:32:20.503474Z","shell.execute_reply.started":"2023-05-14T15:32:20.495115Z","shell.execute_reply":"2023-05-14T15:32:20.502055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass UNet(nn.Module):\n    def __init__(self,in_channels = 3,out_classes = 34, features=[64,128,256,512]):\n        super(UNet, self).__init__()\n        \n        self.ups=nn.ModuleList()\n        self.downs=nn.ModuleList()\n        self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        for feature in features:\n            self.downs.append(\n                DoubleConv\n                    (in_channels,feature))\n            in_channels=feature\n        \n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d\n                            (2*feature,feature,kernel_size=2,stride=2)\n                           )\n            self.ups.append(DoubleConv(feature*2,feature))\n            \n        self.bootleneck = DoubleConv(features[-1], features[-1]*2)\n        \n        self.final_conv = nn.Conv2d(features[0],out_classes,kernel_size=1)\n        \n    def forward(self, x):\n        skip_connections = []\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n        x = self.bootleneck(x)\n        skip_connections = skip_connections[::-1]\n        for idx in range(0,len(self.ups),2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x,size = skip_connection.shape[2:])\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n        x = self.final_conv(x)\n\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:32:22.056527Z","iopub.execute_input":"2023-05-14T15:32:22.056889Z","iopub.status.idle":"2023-05-14T15:32:22.069723Z","shell.execute_reply.started":"2023-05-14T15:32:22.056859Z","shell.execute_reply":"2023-05-14T15:32:22.068269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing dimension\ndef test():\n    x=torch.randn((1,3,256,256))\n    print(x.shape)\n    model = UNet(3,34)\n    preds = model(x)\n    print(preds.shape)\n    \ntest()    ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T14:24:44.184909Z","iopub.execute_input":"2023-05-14T14:24:44.18521Z","iopub.status.idle":"2023-05-14T14:24:45.946217Z","shell.execute_reply.started":"2023-05-14T14:24:44.185184Z","shell.execute_reply":"2023-05-14T14:24:45.944219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definisci il numero di epoche e il learning rate\n\nepochs = 4\nlearning_rate = 5e-4\n\n# e il dispositivo su cui far girare il codice ('cuda' o 'cpu')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Definire la loss function, che può essere custom o presa dal pacchetto nn\nbatch_size = 32\nmodel = UNet(3,34)\nmodel = model.to(device)\n\n\n# definire l'ottimizzatore\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:32:25.78712Z","iopub.execute_input":"2023-05-14T15:32:25.78748Z","iopub.status.idle":"2023-05-14T15:32:31.497699Z","shell.execute_reply.started":"2023-05-14T15:32:25.78745Z","shell.execute_reply":"2023-05-14T15:32:31.496733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get train and val dataset instances\ntrain_dir = '/kaggle/input/cityscapes/Cityspaces/images/train'\nlabel_train_dir = '/kaggle/input/cityscapes/Cityspaces/gtFine/train'\nval_dir = '/kaggle/input/cityscapes/Cityspaces/images/val'\nlabel_val_dir = '/kaggle/input/cityscapes/Cityspaces/gtFine/val'\n\ntrain_dataset = CityscapesDataset(image_dir=train_dir, label_dir=label_train_dir)\nvalid_dataset = CityscapesDataset(image_dir=val_dir, label_dir=label_val_dir)\n\n\ntrain_dataset, test_train_dataset = torch.utils.data.random_split(train_dataset, [0.8, 0.2])\nvalid_dataset, test_valid_dataset = torch.utils.data.random_split(valid_dataset, [0.8, 0.2])\n\ntest_dataset = test_train_dataset + test_valid_dataset\n\n#divisione del dataset da aggiustare\n#train_dataset.__getitem__(3)    \n# Get train and val data loaders\ntest_loader = DataLoader(test_dataset)\ntrain_loader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(valid_dataset,batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T17:06:31.708644Z","iopub.execute_input":"2023-05-14T17:06:31.709003Z","iopub.status.idle":"2023-05-14T17:06:31.944853Z","shell.execute_reply.started":"2023-05-14T17:06:31.708975Z","shell.execute_reply":"2023-05-14T17:06:31.943928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#You could skip this cell, some weights are already calculated","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_distr(train_loader, val_loader):\n    #To calculate the distribution of the classes in the dataset\n    #you can count all the pixels of the same class and divide it \n    #by the length\n    size_t = len(train_loader)\n    size_v = len(val_loader)\n    hist = np.zeros(34)\n    tot_pix =(size_t+size_v)*(110*220)\n    for img,mask,colored in tqdm(train_loader):\n        mask = mask.flatten()\n        histo, bins = np.histogram(mask, 34, [0, 33])\n        print(histo.sum())\n        hist = hist+histo\n        print(hist.sum())\n        \n    for img,mask,colored in tqdm(val_loader):\n        mask = mask.flatten()\n        histo, bins = np.histogram(mask, 34, [0, 33])\n        hist = hist+histo \n    \n    print(hist)\n    \n    for idx in range(len(hist)):\n        hist[idx] = hist[idx]/tot_pix   \n        \n    return hist    \n        \n    \ndist = calculate_distr(train_loader, val_loader)    ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:12:59.321291Z","iopub.execute_input":"2023-05-14T15:12:59.32163Z","iopub.status.idle":"2023-05-14T15:13:49.905296Z","shell.execute_reply.started":"2023-05-14T15:12:59.321601Z","shell.execute_reply":"2023-05-14T15:13:49.903406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size_t = len(train_loader)*32\nsize_v = len(val_loader)*32\nhist = np.zeros(34)\ntot_pix =(size_t+size_v)*(110*220)\nhist = np.array([1.0053000e+04,2.9283580e+06,7.3301200e+05,1.0937090e+06,1.2609740e+06,2.3681800e+05,1.1808730e+06,2.1908385e+07,3.5507740e+06,4.2689300e+05,1.3556500e+05,1.3228245e+07, 4.4057000e+05, 5.3452800e+05, 4.1970000e+04,\n 1.9328200e+05, 6.4073000e+04, 7.1191400e+05, 4.3655000e+04, 1.5305400e+05,\n 3.6562900e+05, 9.4059210e+06, 6.5664500e+05, 2.2735720e+06, 7.1921900e+05,\n 9.5546000e+04, 4.0447800e+06, 1.6719500e+05, 1.6077100e+05, 2.5355000e+04,\n 1.4674000e+04, 1.4209600e+05, 5.6712000e+04, 2.7118000e+05])\n\nfor idx in range(len(hist)):\n        hist[idx] = hist[idx]/tot_pix\nhist = hist*100\nweight=[]\nfor idx, value in enumerate(hist):\n    weight.append(0.01/value)\nweight[0] = 0   \nweight = torch.tensor(weight).float()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:36:43.616648Z","iopub.execute_input":"2023-05-14T15:36:43.617631Z","iopub.status.idle":"2023-05-14T15:36:43.628105Z","shell.execute_reply.started":"2023-05-14T15:36:43.617588Z","shell.execute_reply":"2023-05-14T15:36:43.627006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.CrossEntropyLoss(ignore_index=255, weight=weight)\nloss_function = loss_function.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:36:46.189534Z","iopub.execute_input":"2023-05-14T15:36:46.18989Z","iopub.status.idle":"2023-05-14T15:36:46.197152Z","shell.execute_reply.started":"2023-05-14T15:36:46.189861Z","shell.execute_reply":"2023-05-14T15:36:46.195582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_loss(true, logits, eps=1e-7):\n    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n    Note that PyTorch optimizers minimize a loss. In this\n    case, we would like to maximize the jaccard loss so we\n    return the negated jaccard loss.\n    Args:\n        true: a tensor of shape [B, H, W] or [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        eps: added to the denominator for numerical stability.\n    Returns:\n        jacc_loss: the Jaccard loss.\n    \"\"\"\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1).to(device)[true.squeeze(1).long()]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        true_1_hot = torch.eye(num_classes).to(device)[true.squeeze(1).long()]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return jacc_loss","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:34:01.989895Z","iopub.execute_input":"2023-05-14T15:34:01.99048Z","iopub.status.idle":"2023-05-14T15:34:02.003161Z","shell.execute_reply.started":"2023-05-14T15:34:01.990438Z","shell.execute_reply":"2023-05-14T15:34:02.0021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n#si deve aggiustare IoU\ndef train(epochs, model, train_loader, val_loader, loss, optimizer, patch=False):\n    train_losses = []\n    test_losses = []\n    val_loss = []\n    val_iou = []\n    train_iou = []\n    min_loss = np.inf\n    \n    model.to(device)\n    fit_time = time.time()\n    \n    for e in range(epochs):\n        since = time.time()\n        running_loss = 0\n        iou_score = 0\n        \n        #training loop\n        model.train()\n        for data, target in tqdm(train_loader):\n            #training phase\n\n            mask= target.squeeze()\n            data = data.to(device)\n            mask = mask.to(device)\n            \n            optimizer.zero_grad()\n            #forward\n            \n            output = model(data)\n            \n            # Loss and IoU evaluation\n            loss = loss_function(output, mask)\n            iou_score += jaccard_loss(mask, output).item()\n            # backward\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n\n        model.eval()\n        val_loss = 0\n        val_iou_score = 0\n        #validation loop\n        with torch.no_grad():\n            for data, target in tqdm(val_loader):\n                output = model(data.to(device))\n                mask= target.squeeze()\n\n                mask = mask.to(device)\n                \n                #Loss and IoU evaluation\n                val_iou_score +=  jaccard_loss(mask, output)\n                loss = loss_function(output, mask)                          \n                val_loss += loss.item()\n            \n        # Viene salvata la loss ad ogni epoca sia per il training che per il validation\n        train_losses.append(running_loss/len(train_loader))\n        test_losses.append(val_loss/len(val_loader))\n\n        # Viene salvato il modello solo se la validation loss è migliore degli step precedenti.\n        \n        if min_loss > (val_loss/len(val_loader)):\n            print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (val_loss/len(val_loader))))\n            min_loss = (val_loss/len(val_loader))\n\n            print('saving model...')\n            torch.save(model.state_dict(), 'UNet_best.pt')\n\n\n        # Viene salvata la IoU ad ogni epoca sia per il training che per il validation\n        val_iou.append(val_iou_score/len(val_loader))\n        train_iou.append(iou_score/len(train_loader))\n\n        print(\"Epoch:{}/{}..\".format(e+1, epochs),\n              \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n              \"Val Loss: {:.3f}..\".format(val_loss/len(val_loader)),\n              \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n              \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n              \"Time: {:.2f}m\".format((time.time()-since)/60))\n        \n    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n               'train_miou' :train_iou, 'val_miou':val_iou,\n              }\n    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n    print('saving model...')\n    torch.save(model, 'UNet_last.pt')\n    return history\n\nhistory = train(epochs, model, train_loader, val_loader, loss_function, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T15:40:48.520546Z","iopub.execute_input":"2023-05-14T15:40:48.520889Z","iopub.status.idle":"2023-05-14T16:50:27.336325Z","shell.execute_reply.started":"2023-05-14T15:40:48.52086Z","shell.execute_reply":"2023-05-14T16:50:27.335315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_idx = random.randint(0, len(test_loader)-1)\nimage, mask, colored = test_dataset[random_idx]\n\ntest_iou_score = 0\nmodel = UNet(3,34)\nmodel.load_state_dict(torch.load('/kaggle/working/UNet_best.pt'))\nmodel.eval()\nwith torch.no_grad():\n    for idx , (data, target, colored) in enumerate(test_loader):\n\n        image = data\n\n        output = model(data)\n        \n        predictions = torch.nn.functional.softmax(output, dim=1)\n        pred_labels = torch.argmax(predictions, dim=1) \n        pred_labels = pred_labels.float()\n        pred_labels = pred_labels.squeeze()\n\n        #IoU evaluation\n        test_iou = jaccard_loss(target, output)\n        \n        image = image.squeeze()\n        colored = colored.squeeze()\n        im = image[0].cpu().numpy()\n        \n        mask = colored[0].cpu().numpy()\n        \n        out = pred_labels.cpu().numpy()\n        \n\n        plt.figure()\n        fig, axs = plt.subplots(1,3,sharex=True, sharey=True,figsize=(8,3))\n        axs[0].imshow(im)\n        axs[0].set_title('Immagine RGB', fontsize=10, fontweight='bold')\n        axs[1].imshow(mask)\n        axs[1].set_title('Ground truth', fontsize=10, fontweight='bold')\n        axs[2].imshow(out)\n        axs[2].set_title('Prediction', fontsize=10, fontweight='bold')\n        \n        if idx%5==0:\n            plt.close('all')\n\n        \n        print(f\"Test IoU = {test_iou}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-14T17:08:19.032905Z","iopub.execute_input":"2023-05-14T17:08:19.033485Z","iopub.status.idle":"2023-05-14T17:08:25.917235Z","shell.execute_reply.started":"2023-05-14T17:08:19.033452Z","shell.execute_reply":"2023-05-14T17:08:25.915125Z"},"trusted":true},"execution_count":null,"outputs":[]}]}